---
title: LLMs Will Fragment — Together with Humanity
date: 2026-02-24
authors:
  - me

image:
  caption: ''
  focal_point: 'Center'
  preview_only: true

tags:
  - LLM
  - AI
  - Coarsening
  - Impossibility
  - Essay

content_meta:
  trending: true
---

When DeepSeek arrived, the Western AI world was shaken. I was not.

To explain why requires a detour.

---

{{< toc mobile_only=true is_open=true >}}

## On Coarsening

We do not see the world as it is.

The color spectrum is continuous, but we call things red, blue, green. Political positions form a continuum, but we split them into left and right. The actions of others carry endless context, yet we reduce them to good or evil.

I call this operation *coarsening* — formally, a map ρ from an objective position space L to a label space Σ. Coarsening loses information. Two people at genuinely different positions get folded into the same label. Two people who are close get pulled apart by a boundary they never drew.

The question I have been working on is this: can a given pattern of relations — who can connect with whom — be exactly reproduced using only the labels that coarsening produces?

Sometimes it cannot. When the relational pattern is not constant within the fibers of ρ — within the sets of positions that share the same label — no label-level rule can match it without error. False positives and false negatives. Apparent cohesion where there is none; apparent fragmentation where things are actually close. This is not a design flaw. It is structural.

---

## There Seem to Be Two Kinds of Coarsening

Attributes like coordinates or nationality, and evaluations like justice or evil, are both called coarsenings — but their nature is fundamentally different.

In the first case, a shared underlying space exists. The problem is resolution, and refinement is possible — up to a point. That limit never fully disappears. Cognition has granularity. Observation has floors. But the space itself is common ground.

In the second case, ρ itself differs between people. Person A and person B may use the same label, yet the structure of what each includes under that label is entirely different. This is no longer a question of resolution. They are measuring different things.

These two kinds tend to arrive together. The fact that someone performed action X is a shared 0/1. The label that action X is *evil* is the output of a map — and each person runs their own. The act is observable. The evaluation is not.

This raises a question that has stayed with me: when a single rule Rb is chosen, whose ρ is it built from?

---

## What LLMs Are Actually Choosing

Large language models learn from vast human text, extract personas from it, and through post-training, refine those personas in a particular direction. Anthropic recently published a paper calling this the persona selection model.

Translated into my framework: an LLM is selecting a single Rb. That Rb is optimized around a particular ρ. There is no escaping the question of whose feedback shapes it. The ρ of Western liberal democracy — and more specifically, the cultural ρ of Silicon Valley — is what currently anchors the label-level rules of the dominant models.

I am not inclined to call this malicious. It is structural. Once you choose a single Rb, errors for someone are unavoidable. And that Rb being pulled toward some ρ rather than none is equally unavoidable. The reason "neutral AI" cannot exist is not ethical. It is mathematical.

---

## A Forecast

From DeepSeek's emergence, I see a trajectory.

China is building its own models and insisting that each nation should develop AI aligned to its own values. This is not primarily a technology competition. It is a disagreement about ρ. A model built to embody "socialist core values" and one built around "individual autonomy and free expression" will return different labels for the same observable act. My theory says nothing about which is correct. It says only that both are choosing a single Rb.

This will accelerate, and not only in China. Governments, religious communities, political factions, subcultures — each will move toward models optimized around their own ρ. And within each of those, fragmentation will continue. The question of what counts as conservative, or feminist, or orthodox does not resolve once a label is shared. The fiber structure — what the label actually contains — remains perpetually contested.

When a community moves toward the lower approximation — refining conditions, keeping only what its own ρ can assert with certainty — there is a kind of integrity to it. But the more precisely the conditions are stated, the more plainly it looks like a lie from any position that does not share them. To say anything is to divide the world. Move toward the upper approximation — try to hold every position — and the result says nothing at all. There is no exit in either direction.

---

## On Ambiguity

What I keep thinking about is the region B.

B sits between the lower and upper approximations — the zone where no label-level rule can render a definitive answer. The standard treatment is to regard this as a problem. Removing ambiguity is the goal.

But B cannot be emptied. The structure will not allow it.

Seen from the other side: B is the territory that remains undecided for everyone. Neither A's ρ nor B's ρ can force an answer here. That makes it the only genuinely common ground — the place where no one has yet staked a claim, the place where no ρ is fully confirmed.

The history of legislation shows something about this. Legal boundaries always arrive late. An event happens, harm becomes visible, and only then does a line get drawn through part of B. This delay is widely criticized as a failure of the system. To me it looks like evidence that B is structurally persistent. Every time a boundary is drawn, a new B opens somewhere else. The errors do not disappear. They relocate.

As long as ambiguity is treated as a defect to be eliminated, fragmentation will not stop. Every faction will spend its energy trying to collapse B. The time and attention of humanity will flow there.

If B could be held as a place of negotiation rather than a problem to be solved — if the agreement that *this is not yet decided* carried as much weight as the agreement that *this is so* — something different might be possible.

I do not believe this is easy. For finite beings whose survival depends on maintaining their own ρ, holding ambiguity open carries real cost. But whether it is feasible and whether it is the only non-trivial response are separate questions.

---

## This Text Also Divides the World

Writing this, I notice something.

The act of writing it is already a coarsening. The moment I frame the proposition that LLMs will fragment, I have taken a position. Some will agree. Others will not. That is not a failure of the argument. It is the structural consequence of saying anything at all.

To speak is to divide. To take a position is to become an error for someone who does not share it. To stay silent is to say nothing. This is not a property of theories about language. It is a property of every act of expression.

So what I am trying to say here is not that a theory is correct.

It is a question: what does it mean to speak, knowing that speaking always draws a line? Is it possible to hold ambiguity — and still not be silent?

I do not have an answer. Only this: no attempt to dissolve ambiguity has ever succeeded. And I do not expect the next one will either.

---

*Related papers: "The Impossibility of Cohesion Without Fragmentation" (2026), "Space Reduction and Representability of Relational Feasibility" (2026), Daisuke Hirota, arXiv / Zenodo*